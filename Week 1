# Citation
Hastie, et al. Elements of Statistical Learning, Figure 13.2

# Problem setting
What is the input:
The input should include lattice points of all x, the coordinate of training set, the
coordinate of the supervise set, the y value of each point, and the probability at each
lattice points.
What is the output:
The decision boundary generated by Bayes model.
The boundary generated by K-mean algorithm.

# Data sources paragraph:
I would like to use the data that provided by the book, this data set is simulated data
and it is not real-world data, the data set can get from
https://web.stanford.edu/~hastie/ElemStatLearn/datasets/mixture.example.info.txt
There are 8 rows in the data set.
The data set includes:
x: 200 * 2 matrix of training predictors
y: Class variable: logical vector of TRUES and FALSES â€“ 100 of each
xnew: matrix 6831 * 2 of lattice points in predictor space
prob: vector of 6831 probabilities (of class TRUE) at each lattice point
marginal: marginal probability at each lattice point
px1: 69 lattice coordinates for x.1
px2: 99 lattice value for x.2 (69 * 99 =6831)
means: 20 * 2 matrix of mixture centers, first ten for one class, next ten for the
other.

# Paragraph describing the algorithm studied:
i) K-means algorithm alternates the two steps:
1) For each center we identify the subset of training points that is closer to
it than any other center.
2) The means of each feature for the data points in each cluster are
computed, and this mean vector becomes the new center for the cluster.
These two steps are iterated until convergence. The initial centers are R
randomly chosen observations from the training data.
ii) To use K-means clustering for classification of labeled data, the steps are:
1) Apply K-means clustering to the training data in each class separately,
using R prototypes per class.
2) Assign a class label to each of the K * R prototypes.
3) Classify a new feature x to the class of the closest prototype.
